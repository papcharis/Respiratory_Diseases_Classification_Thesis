{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import glob\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchaudio\n",
    "from IPython.display import Audio\n",
    "from torch.utils.data import DataLoader, Dataset, random_split, TensorDataset\n",
    "import numpy as np\n",
    "import librosa.display\n",
    "# import importlib\n",
    "# import seaborn as sns\n",
    "from scipy.interpolate import UnivariateSpline\n",
    "from useful_functions import plot_wav, plot_fft\n",
    "import pickle\n",
    "import torch.nn as nn\n",
    "import math\n",
    "import torch.nn.functional as F\n",
    "from torch.nn import init\n",
    "import importlib\n",
    "import scipy\n",
    "import random\n",
    "import sklearn\n",
    "import resampy\n",
    "import tqdm\n",
    "import nlpaug\n",
    "import nlpaug.augmenter.audio as naa\n",
    "import torch.optim as optim\n",
    "from scipy.signal import butter, lfilter\n",
    "import matplotlib.pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining some of the main parameters used in the project\n",
    "SR = 4000\n",
    "LENGTH = 8000\n",
    "CUT_OFF_FREQ = 80\n",
    "FILTER_ORDER = 6\n",
    "N_MELS = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/mnt/c/Users/papch/OneDrive/Υπολογιστής/Σχολή/Internship-Thesis/ICBHI_final_database'"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "path = \"/mnt/c/Users/papch/OneDrive/Υπολογιστής/Σχολή/Internship-Thesis/ICBHI_final_database\"\n",
    "os.chdir(path)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "icbhi = pd.read_csv(\"ICBHI_2017_Breath_Cycles.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating multiple audio signal processing functions. A lot of them were not used after all\n",
    "class AudioUtil():\n",
    "    def open(df, index, sr):\n",
    "        sig, sr = librosa.load(df.iloc[index][0], sr = sr, offset = df.iloc[index][1], duration = df.iloc[index][2] - df.iloc[index][1]) #audio = AudioUtil.open(icbhi.iloc[1][0], icbhi.iloc[1][1], icbhi.iloc[1][2]) to call it\n",
    "        return (sig,sr)\n",
    "    \n",
    "    def resample(audio, newSR):\n",
    "        sig, sr = audio\n",
    "        \n",
    "        if(sr == newSR):\n",
    "            return audio\n",
    "\n",
    "        resig = torchaudio.transforms.Resample(sr, newSR)(torch.from_numpy(sig))\n",
    "\n",
    "        return (resig.numpy(), newSR)\n",
    "\n",
    "    def high_pass_filter(audio, cut_off_freq, filter_order):\n",
    "        sig, sr = audio\n",
    "\n",
    "        nyquist = sr/2\n",
    "       \n",
    "        b, a = scipy.signal.butter(filter_order, cut_off_freq/nyquist, btype='highpass')\n",
    "\n",
    "        output_signal = scipy.signal.filtfilt(b, a, sig)\n",
    "\n",
    "        return(output_signal, sr)\n",
    "\n",
    "\n",
    "    def zeropad(audio, length):\n",
    "        sig, sr = audio\n",
    "        max_len = int((length * sr)/1000)\n",
    "\n",
    "        if (len(sig) >= max_len):\n",
    "            new_sig = sig[:max_len]\n",
    "        elif (len(sig) < max_len):\n",
    "            pad_begin = random.randint(0,max_len - len(sig))\n",
    "            pad_end = max_len-len(sig)-pad_begin\n",
    "            new_sig= np.pad(sig, (pad_begin,pad_end), mode='constant')\n",
    "            # new_sig = torch.cat(pad_begin, sig, pad_end)\n",
    "        # elif (len(sig) < max_len):\n",
    "        #     new_sig = np.empty(0)\n",
    "        #     no_repeats = max_len // len(sig)\n",
    "        #     end_pad = max_len - no_repeats*len(sig)\n",
    "        #     for i in range(no_repeats):\n",
    "        #         new_sig = np.concatenate([new_sig, sig])\n",
    "            \n",
    "        #     new_sig = np.concatenate([new_sig, sig[:end_pad]])\n",
    "\n",
    "\n",
    "        if (torch.is_tensor(new_sig)):\n",
    "            new_sig = new_sig.numpy()\n",
    "            \n",
    "        return (new_sig, sr)\n",
    "\n",
    "    def interpolate(audio, length_ms):\n",
    "        sig, sr = audio\n",
    "\n",
    "        old_indices = np.arange(0, len(sig))\n",
    "        new_length = (sr*length_ms)/1000\n",
    "        new_indices = np.linspace(0, len(sig)-1, int(new_length))\n",
    "        \n",
    "        spl = UnivariateSpline(old_indices, sig, s=0)\n",
    "        new_sig = spl(new_indices)\n",
    "        return (new_sig, sr)\n",
    "\n",
    "    def zero_and_interpol(audio, length):\n",
    "        sig, sr = audio\n",
    "        max_len = int((length*sr)/1000)\n",
    "\n",
    "        if (len(sig) > max_len):\n",
    "            old_indices = np.arange(0, len(sig))\n",
    "            new_indices = np.linspace(0, len(sig)-1, int(max_len))\n",
    "\n",
    "            spl = UnivariateSpline(old_indices, sig, s=0)\n",
    "            new_sig = spl(new_indices)\n",
    "\n",
    "        elif (len(sig) <= max_len):\n",
    "            pad_begin = random.randint(0, max_len - len(sig))\n",
    "            pad_end = max_len - len(sig) - pad_begin\n",
    "            new_sig = np.pad(sig, (pad_begin, pad_end), mode = 'constant')\n",
    "\n",
    "        return (new_sig, sr)\n",
    "\n",
    "\n",
    "    def time_shift(audio, shift_limit):\n",
    "        sig, sr = audio\n",
    "        sig_len = len(sig)\n",
    "        shift_amt = int(random.random() * shift_limit * sig_len)\n",
    "        return (np.roll(sig, shift_amt), sr)\n",
    "\n",
    "    def hannWindow(audio, length, sr):\n",
    "        sig, sr = audio\n",
    "        window = np.hanning(len(sig))\n",
    "\n",
    "        new_sig = (sig * window).astype(np.float32)\n",
    "        return(new_sig, sr)\n",
    "\n",
    "\n",
    "    def librosaMelSpectro(audio):\n",
    "        sig, sr = audio\n",
    "\n",
    "        mel_spectrogram = librosa.feature.melspectrogram(sig, sr=sr, n_fft=256, hop_length=128, n_mels=N_MELS)\n",
    "        log_mel_spectrogram = librosa.power_to_db(mel_spectrogram)\n",
    "\n",
    "        return log_mel_spectrogram\n",
    "\n",
    "    def calculateMFCCS(spectro):\n",
    "        mfccs = librosa.feature.mfcc(S=spectro, n_mfcc=40)\n",
    "        return mfccs\n",
    "\n",
    "    def calculateDeltaMFCCS(mfccs):\n",
    "        delta_mfccs = librosa.feature.delta(mfccs)\n",
    "        return delta_mfccs\n",
    "\n",
    "    def calculateDelta2MFCCS(mfccs):\n",
    "        delta2_mfccs = librosa.feature.delta(mfccs, order=2)\n",
    "        return delta2_mfccs\n",
    "\n",
    "    def cwt(audio):\n",
    "        sig, sr = audio\n",
    "        coeff, _ = pywt.cwt(sig, range(1, 5), 'morl', sampling_period=1/sr)\n",
    "        \n",
    "        return coeff\n",
    "\n",
    "    def normalization(audio):\n",
    "        sig, sr = audio\n",
    "\n",
    "        new_sig = (sig-min(sig))/(max(sig)-min(sig))\n",
    "        # new_sig = (sig-np.mean(sig))/np.std(sig)\n",
    "\n",
    "        return(new_sig, sr)\n",
    "\n",
    "    def information_entropy(audio):\n",
    "        sig, sr = audio\n",
    "\n",
    "        # z = np.arange(min(sig), max(sig), 0.1)\n",
    "        inst, bins = np.histogram(sig, range = (min(sig), max(sig)))\n",
    "        ent_ppg = 0\n",
    "\n",
    "        for i in range(len(inst)):\n",
    "            if inst[i] != 0:\n",
    "                p_z = inst[i]/sum(inst)\n",
    "                ent_ppg = (-p_z)*math.log2(p_z) + ent_ppg\n",
    "        \n",
    "        return ent_ppg\n",
    "\n",
    "    def masking(spec, n_time_masks, n_freq_masks):\n",
    "        n_mels, n_steps = spec.shape\n",
    "        mask_value = spec.mean()\n",
    "        spec_aug = spec\n",
    "\n",
    "        freq_mask_limit = 0.1 * n_mels\n",
    "\n",
    "        for _ in range(n_freq_masks):\n",
    "            freq_mask_length = int(random.random() * freq_mask_limit)\n",
    "            freq_mask_start = random.randint(0, n_mels)\n",
    "            \n",
    "            if ((freq_mask_start + freq_mask_length) < n_mels):\n",
    "                spec_aug[freq_mask_start:freq_mask_start+freq_mask_length, :] = mask_value\n",
    "            else:\n",
    "                spec_aug[freq_mask_start:n_steps, :] = mask_value\n",
    "        \n",
    "        time_mask_limit = 0.1 * n_steps\n",
    "\n",
    "        for _ in range(n_time_masks):\n",
    "            time_mask_length = int(random.random() * time_mask_limit)\n",
    "            time_mask_start = random.randint(0, n_steps)\n",
    "\n",
    "            if ((time_mask_start + time_mask_length) < n_steps):\n",
    "                spec_aug[:, time_mask_start:time_mask_start+time_mask_length] = mask_value\n",
    "            else:\n",
    "                spec_aug[:, time_mask_start:n_steps] = mask_value\n",
    "\n",
    "        return spec_aug\n",
    "\n",
    "    def feature_norm(feature):\n",
    "        new_feat = (feature-glb_mean)/glb_std\n",
    "        return new_feat\n",
    "\n",
    "    def spectrogram_alternate(audio):\n",
    "        audio, sr = audio\n",
    "\n",
    "        frame_size = 0.025\n",
    "        frame_stride = 0.01\n",
    "\n",
    "        frame_length, frame_step = frame_size * sr, frame_stride * sr\n",
    "        signal_length = len(audio)\n",
    "        frame_length = int(round(frame_length))\n",
    "        frame_step = int(round(frame_step))\n",
    "        num_frames = int(np.ceil(float(np.abs(signal_length - frame_length)) / frame_step))\n",
    "\n",
    "        pad_signal_length = num_frames * frame_step + frame_length\n",
    "        z = np.zeros((pad_signal_length - signal_length))\n",
    "        pad_signal = np.append(audio, z)\n",
    "\n",
    "        indices = np.tile(np.arange(0, frame_length), (num_frames, 1)) + np.tile(np.arange(0, num_frames * frame_step, frame_step), (frame_length, 1)).T\n",
    "        frames = pad_signal[indices.astype(np.int32, copy=False)]\n",
    "\n",
    "        frames *= np.hamming(frame_length)\n",
    "\n",
    "        NFFT = 512\n",
    "        mag_frames = np.absolute(np.fft.rfft(frames, NFFT))  # Magnitude of the FFT\n",
    "        pow_frames = ((1.0 / NFFT) * ((mag_frames) ** 2))  # Power Spectrum\n",
    "\n",
    "        nfilt = 40\n",
    "\n",
    "        low_freq_mel = 0\n",
    "        high_freq_mel = (2595 * np.log10(1 + (sr / 2) / 700))  # Convert Hz to Mel\n",
    "        mel_points = np.linspace(low_freq_mel, high_freq_mel, nfilt + 2)  # Equally spaced in Mel scale\n",
    "        hz_points = (700 * (10**(mel_points / 2595) - 1))  # Convert Mel to Hz\n",
    "        bin = np.floor((NFFT + 1) * hz_points / sr)\n",
    "\n",
    "        fbank = np.zeros((nfilt, int(np.floor(NFFT / 2 + 1))))\n",
    "        for m in range(1, nfilt + 1):\n",
    "            f_m_minus = int(bin[m - 1])   # left\n",
    "            f_m = int(bin[m])             # center\n",
    "            f_m_plus = int(bin[m + 1])    # right\n",
    "\n",
    "            for k in range(f_m_minus, f_m):\n",
    "                fbank[m - 1, k] = (k - bin[m - 1]) / (bin[m] - bin[m - 1])\n",
    "            for k in range(f_m, f_m_plus):\n",
    "                fbank[m - 1, k] = (bin[m + 1] - k) / (bin[m + 1] - bin[m])\n",
    "        filter_banks = np.dot(pow_frames, fbank.T)\n",
    "        filter_banks = np.where(filter_banks == 0, np.finfo(float).eps, filter_banks)  # Numerical Stability\n",
    "        filter_banks = 20 * np.log10(filter_banks)  # dB\n",
    "\n",
    "        return filter_banks.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating a butterwoth band-pass filter function\n",
    "def butter_bandpass(lowcut, highcut, fs, order=5):\n",
    "    nyq = 0.5 * fs\n",
    "    low = lowcut / nyq\n",
    "    high = highcut / nyq\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return b, a\n",
    "\n",
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    b, a = butter_bandpass(lowcut, highcut, fs, order=order)\n",
    "    y = lfilter(b,a, data)\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Extract_Annotation_Data(file_name, data_dir):\n",
    "    tokens = file_name.split('_')\n",
    "    recording_info = pd.DataFrame(data = [tokens], columns = ['Patient Number', 'Recording index', 'Chest location','Acquisition mode','Recording equipment'])\n",
    "    recording_annotations = pd.read_csv(os.path.join(data_dir, file_name + '.txt'), names = ['Start', 'End', 'Crackles', 'Wheezes'], delimiter= '\\t')\n",
    "    return recording_info, recording_annotations\n",
    "\n",
    "# get annotations data and filenames\n",
    "def get_annotations(data_dir):\n",
    "\tfilenames = [s.split('.')[0] for s in os.listdir(data_dir) if '.wav' in s]\n",
    "\ti_list = []\n",
    "\trec_annotations_dict = {}\n",
    "\tfor s in filenames:\n",
    "\t\ti,a = Extract_Annotation_Data(s, data_dir)\n",
    "\t\ti_list.append(i)\n",
    "\t\trec_annotations_dict[s] = a\n",
    "\n",
    "\trecording_info = pd.concat(i_list, axis = 0)\n",
    "\trecording_info.head()\n",
    "\n",
    "\treturn filenames, rec_annotations_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_sound_samples(recording_annotations, file_name, data_dir, sample_rate):\n",
    "    sample_data = [file_name]\n",
    "    # load file with specified sample rate (also converts to mono)\n",
    "    data, rate = librosa.load(os.path.join(data_dir, file_name+'.wav'), sr=sample_rate)\n",
    "    data, rate = AudioUtil.normalization((data, rate))\n",
    "    # data, rate = AudioUtil.high_pass_filter((data, rate), CUT_OFF_FREQ, FILTER_ORDER)\n",
    "    data = butter_bandpass_filter(data, 80, 1800, sample_rate)\n",
    "    #print(\"Sample Rate\", rate)\n",
    "    \n",
    "    for i in range(len(recording_annotations.index)):\n",
    "        row = recording_annotations.loc[i]\n",
    "        start = row['Start']\n",
    "        end = row['End']\n",
    "        crackles = row['Crackles']\n",
    "        wheezes = row['Wheezes']\n",
    "        audio_chunk = slice_data(start, end, data, rate)\n",
    "        sample_data.append((audio_chunk, start,end, get_label(crackles, wheezes)))\n",
    "    return sample_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Segment audio samples into breath cycles\n",
    "def slice_data(start, end, raw_data, sample_rate):\n",
    "    max_ind = len(raw_data) \n",
    "    start_ind = min(int(start * sample_rate), max_ind)\n",
    "    end_ind = min(int(end * sample_rate), max_ind)\n",
    "    return raw_data[start_ind: end_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Returning the label of each breath cycle\n",
    "def get_label(crackle, wheeze):\n",
    "    if crackle == 0 and wheeze == 0:\n",
    "        return 0\n",
    "    elif crackle == 1 and wheeze == 0:\n",
    "        return 1\n",
    "    elif crackle == 0 and wheeze == 1:\n",
    "        return 2\n",
    "    else:\n",
    "        return 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Padding the samples to the specific window length chosen\n",
    "def generate_padded_samples(original, source, output_length, sample_rate):\n",
    "    copy = np.zeros(output_length, dtype=np.float32)\n",
    "    src_length = len(source)\n",
    "    pad = output_length-src_length \n",
    "\n",
    "    prob= random.random()\n",
    "    aug = original\n",
    "\n",
    "    while len(aug) < pad:\n",
    "        aug = np.concatenate([aug,aug])\n",
    "\n",
    "    if prob < 0.5:\n",
    "        copy[pad:] = source\n",
    "        copy[:pad] = aug[len(aug)-pad:]\n",
    "    else:\n",
    "        copy[:src_length] = source[:]\n",
    "        copy[src_length:] = aug[:pad]\n",
    "    \n",
    "    return copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Choosing the correct padding of each cycle\n",
    "def split_and_pad(original, desiredLength, sample_rate):\n",
    "    output_buffer_length = int(desiredLength*sample_rate/1000)\n",
    "    soundclip = original[0].copy()\n",
    "    n_samples = len(soundclip)\n",
    "\n",
    "    output = []\n",
    "\n",
    "    # if: the audio sample length > desiredLength, then split & pad\n",
    "\t# else: simply pad according to given type 1 or 2\n",
    "    if n_samples > output_buffer_length:\n",
    "        frames = librosa.util.frame(soundclip, frame_length=output_buffer_length, hop_length=output_buffer_length//2, axis=0)\n",
    "        for i in range(frames.shape[0]):\n",
    "            output.append((frames[i], original[1]))\n",
    "\n",
    "        last_id = frames.shape[0]*(output_buffer_length//2)\n",
    "        last_sample = soundclip[last_id:]\n",
    "        padded = generate_padded_samples(soundclip, last_sample, output_buffer_length, sample_rate)\n",
    "        output.append((padded, original[1]))\n",
    "\n",
    "    else:\n",
    "        padded = generate_padded_samples(soundclip, soundclip, output_buffer_length, sample_rate)\n",
    "        output.append((padded, original[1]))\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Applying some data augmentation techniques on the audio samples using the nlpaug library\n",
    "def gen_augmented(original, sample_rate):\n",
    "    augment_list = [\n",
    "        naa.NoiseAug(),\n",
    "        naa.SpeedAug(),\n",
    "        naa.LoudnessAug(factor=(0.5,2)),\n",
    "        naa.VtlpAug(sampling_rate=sample_rate, zone=(0.0,1.0)),\n",
    "        naa.PitchAug(sampling_rate=sample_rate, factor=(-1,3))\n",
    "    ]\n",
    "\n",
    "    aug_idx = random.randint(0, len(augment_list)-1)\n",
    "    augmented_data = augment_list[aug_idx].augment(original)\n",
    "    return augmented_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Rolling the audio\n",
    "def rollAudio(audio):\n",
    "    pivot = np.random.randint(audio.shape[0])\n",
    "    rolled_audio = np.roll(audio, pivot, axis=0)\n",
    "    assert audio.shape[0] == rolled_audio.shape[0], \"Roll audio shape mismatch\"\n",
    "    return rolled_audio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataset\n",
    "class DS(Dataset):\n",
    "    def __init__(self, df, data_path, is_train, aug_scale):\n",
    "        self.df = pd.read_csv(df)\n",
    "        self.data_path = data_path\n",
    "        self.sr = SR\n",
    "        self.length = LENGTH\n",
    "        self.channels = 1\n",
    "        self.cutoff_freq = CUT_OFF_FREQ\n",
    "        self.filter_order = FILTER_ORDER\n",
    "        self.shift_limit = 0.4\n",
    "        self.hop_length = 128\n",
    "        self.is_train = is_train\n",
    "\n",
    "        self.audio_data = []\n",
    "        self.cycle_list = []\n",
    "        self.classwise_cycle_list = [[], [], [], []]\n",
    "\n",
    "        _, rec_annotations_dict = get_annotations(data_path)\n",
    "\n",
    "        filenames = []\n",
    "\n",
    "        if is_train:\n",
    "            with open(\"ICBHI_challenge_train_test.txt\") as file:\n",
    "                lines = file.readlines()\n",
    "                file.close()\n",
    "                lines = [line.rstrip().split() for line in lines]\n",
    "                for line in lines:\n",
    "                    if line[1] == 'train':\n",
    "                        filenames.append(line[0])\n",
    "        else:\n",
    "            with open(\"ICBHI_challenge_train_test.txt\") as file:\n",
    "                lines = file.readlines()\n",
    "                file.close()\n",
    "                lines = [line.rstrip().split() for line in lines]\n",
    "                for line in lines:\n",
    "                    if line[1] == 'test':\n",
    "                        filenames.append(line[0])\n",
    "\n",
    "\n",
    "        for idx, file_name in tqdm.tqdm(enumerate(filenames)):\n",
    "            data = get_sound_samples(rec_annotations_dict[file_name], file_name, data_path, self.sr)\n",
    "            for i in range(len(data[1:])):\n",
    "                self.cycle_list.append((data[i+1][0], data[i+1][3]))\n",
    "                self.classwise_cycle_list[data[i+1][3]].append(data[i+1][0])\n",
    "        \n",
    "        # for i in range(len(self.df)):\n",
    "        #     data = AudioUtil.open(self.df, i, self.sr)\n",
    "        #     data = AudioUtil.high_pass_filter(data, self.cutoff_freq, self.filter_order)\n",
    "        #     label = self.df.iloc[i][3]\n",
    "        #     self.cycle_list.append((data[0], label))\n",
    "        #     self.classwise_cycle_list[self.df.iloc[i][3]].append(data[0])\n",
    "\n",
    "        if is_train and aug_scale:\n",
    "            self.increase_data(scale=aug_scale)\n",
    "\n",
    "        for _, sample in enumerate(self.cycle_list):\n",
    "            output = split_and_pad(sample, self.length, self.sr)\n",
    "            self.audio_data.extend(output)\n",
    "            \n",
    "\n",
    "    #Creating new samples for augmenting the training data - Concatenating already existing samples\n",
    "    def increase_data(self, scale=1):\n",
    "        #augment normal\n",
    "        aug_nos = scale*len(self.classwise_cycle_list[0]) - len(self.classwise_cycle_list[0])\n",
    "        for idx in range(aug_nos):\n",
    "            # normal_i + normal_j\n",
    "            i = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "            j = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "            normal_i = self.classwise_cycle_list[0][i]\n",
    "            normal_j = self.classwise_cycle_list[0][j]\n",
    "            new_sample = np.concatenate([normal_i, normal_j])\n",
    "            self.cycle_list.append((new_sample, 0))\n",
    "\n",
    "        # augment crackle\n",
    "        aug_nos = scale*len(self.classwise_cycle_list[0]) - len(self.classwise_cycle_list[1])\n",
    "        for idx in range(aug_nos):\n",
    "            aug_prob = random.random()\n",
    "\n",
    "            if aug_prob < 0.6:\n",
    "                # crackle_i + crackle_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[1])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[1])-1)\n",
    "                sample_i = self.classwise_cycle_list[1][i]\n",
    "                sample_j = self.classwise_cycle_list[1][j]\n",
    "            elif aug_prob >= 0.6 and aug_prob < 0.8:\n",
    "                # crackle_i + normal_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[1])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "                sample_i = self.classwise_cycle_list[1][i]\n",
    "                sample_j = self.classwise_cycle_list[0][j]\n",
    "            else:\n",
    "                # normal_i + crackle_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[1])-1)\n",
    "                sample_i = self.classwise_cycle_list[0][i]\n",
    "                sample_j = self.classwise_cycle_list[1][j]\n",
    "\n",
    "            new_sample = np.concatenate([sample_i, sample_j])\n",
    "            self.cycle_list.append((new_sample, 1))\n",
    "\n",
    "        # augment wheeze\n",
    "        aug_nos = scale*len(self.classwise_cycle_list[0]) - len(self.classwise_cycle_list[2])\n",
    "        for idx in range(aug_nos):\n",
    "            aug_prob = random.random()\n",
    "\n",
    "            if aug_prob < 0.6:\n",
    "                # wheeze_i + wheeze_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[2])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[2])-1)\n",
    "                sample_i = self.classwise_cycle_list[2][i]\n",
    "                sample_j = self.classwise_cycle_list[2][j]\n",
    "            elif aug_prob >= 0.6 and aug_prob < 0.8:\n",
    "                # wheeze_i + normal_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[2])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "                sample_i = self.classwise_cycle_list[2][i]\n",
    "                sample_j = self.classwise_cycle_list[0][j]\n",
    "            else:\n",
    "                # normal_i + wheeze_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[2])-1)\n",
    "                sample_i = self.classwise_cycle_list[0][i]\n",
    "                sample_j = self.classwise_cycle_list[2][j]\n",
    "\n",
    "            new_sample = np.concatenate([sample_i, sample_j])\n",
    "            self.cycle_list.append((new_sample, 2))\n",
    "\n",
    "        # augment both\n",
    "        aug_nos = scale*len(self.classwise_cycle_list[0]) - len(self.classwise_cycle_list[3])\n",
    "        for idx in range(aug_nos):\n",
    "            aug_prob = random.random()\n",
    "\n",
    "            if aug_prob < 0.5:\n",
    "                # both_i + both_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[3])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[3])-1)\n",
    "                sample_i = self.classwise_cycle_list[3][i]\n",
    "                sample_j = self.classwise_cycle_list[3][j]\n",
    "            elif aug_prob >= 0.5 and aug_prob < 0.7:\n",
    "                # crackle_i + wheeze_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[1])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[2])-1)\n",
    "                sample_i = self.classwise_cycle_list[1][i]\n",
    "                sample_j = self.classwise_cycle_list[2][j]\n",
    "            elif aug_prob >=0.7 and aug_prob < 0.8:\n",
    "                # wheeze_i + crackle_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[2])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[1])-1)\n",
    "                sample_i = self.classwise_cycle_list[2][i]\n",
    "                sample_j = self.classwise_cycle_list[1][j]\n",
    "            elif aug_prob >=0.8 and aug_prob < 0.9:\n",
    "                # both_i + normal_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[3])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "                sample_i = self.classwise_cycle_list[3][i]\n",
    "                sample_j = self.classwise_cycle_list[0][j]\n",
    "            else:\n",
    "                # normal_i + both_j\n",
    "                i = random.randint(0, len(self.classwise_cycle_list[0])-1)\n",
    "                j = random.randint(0, len(self.classwise_cycle_list[3])-1)\n",
    "                sample_i = self.classwise_cycle_list[0][i]\n",
    "                sample_j = self.classwise_cycle_list[3][j]\n",
    "\n",
    "            new_sample = np.concatenate([sample_i, sample_j])\n",
    "            self.cycle_list.append((new_sample, 3))\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.audio_data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        audio, class_id = self.audio_data[idx]\n",
    "        \n",
    "        aug_prob = random.random()\n",
    "        if aug_prob and self.is_train:\n",
    "            audio = gen_augmented(audio, self.sr)\n",
    "            audio = split_and_pad((audio, class_id), self.length, self.sr)[0][0]\n",
    "\n",
    "\n",
    "        roll_prob = random.random()\n",
    "        if roll_prob and self.is_train:\n",
    "            audio = rollAudio(audio)\n",
    "\n",
    "\n",
    "        #Extracting the log-Mel Spectrogram (we have also used the MFCCS, the extraction of them can be seen below)\n",
    "        spectro = AudioUtil.librosaMelSpectro((audio, self.sr))\n",
    "        mfccs = AudioUtil.calculateMFCCS(spectro)\n",
    "        mfccs2 = AudioUtil.calculateDeltaMFCCS(mfccs)\n",
    "        mfccs3 = AudioUtil.calculateDelta2MFCCS(mfccs2)\n",
    "        \n",
    "        return(torch.from_numpy(spectro)[None,:], class_id)\n",
    "        # return(torch.from_numpy(np.concatenate((mfccs,mfccs2,mfccs3), axis=0))[None,:], class_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating our Hybrid CNN-LSTM model and also adding some quantization stubs that will be needed later\n",
    "\n",
    "class Combine2(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Combine2, self).__init__()\n",
    "    \n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size = 5),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size = 3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 96, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm2d(96),\n",
    "            nn.MaxPool2d(kernel_size=2)\n",
    "        )\n",
    "\n",
    "        self.rnn = nn.LSTM(\n",
    "            input_size = 288, #288:Spectro, 1248: 40 MFCCS\"\n",
    "            hidden_size = 64,\n",
    "            num_layers = 1,\n",
    "            batch_first = True)\n",
    "\n",
    "        self.linear = nn.Linear(64, 4)\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "    \n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.dequant(x)\n",
    "\n",
    "        batch_size,  C, H, W = x.size()\n",
    "        timesteps = W\n",
    "    \n",
    "        x = x.contiguous()\n",
    "        r_in = x.view(batch_size, W, -1)\n",
    "        h0 = torch.zeros(1, r_in.size(0), 64).to(device)\n",
    "        c0 = torch.zeros(1, r_in.size(0), 64).to(device)\n",
    "        r_out, (h_n, h_c) = self.rnn(r_in, (h0, c0))\n",
    "        r_out2 = self.linear(r_out[:, -1, :])\n",
    "        \n",
    "        return r_out2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_icbhi = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the training function and editing it so that it can show us the ICBHI Score and the correct predictions after each epoch\n",
    "def training(model, train_dl, val_dl, num_epochs):\n",
    "    global max_icbhi\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    # criterion = FocalLoss(weight = weights)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)  \n",
    "    # optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "    \n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        training_loss = 0.0\n",
    "        validation_loss = 0.0\n",
    "        correct_prediction = 0\n",
    "        val_correct_prediction = 0\n",
    "        total_prediction = 0\n",
    "        val_total_prediction = 0\n",
    "        correct = [0] * 4\n",
    "        total = [0] * 4 \n",
    "\n",
    "\n",
    "        model.train()\n",
    "\n",
    "        for i, data in enumerate(train_dl):\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            # inputs, labels = data[0], data[1]\n",
    "            inputs = inputs.float()\n",
    "\n",
    "            # #z-normalization (may be deleted)\n",
    "            # inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            # inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            # #min-max normalization\n",
    "            # inputs_max, inputs_min = inputs.max(), inputs.min()\n",
    "            # inputs = (inputs - inputs_min)/(inputs_max-inputs_min)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            output_train = model(inputs)\n",
    "\n",
    "            loss_train = criterion(output_train, labels)\n",
    "\n",
    "            loss_train.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            training_loss += loss_train.item()\n",
    "\n",
    "            _, prediction = torch.max(output_train, 1)\n",
    "            \n",
    "            correct_prediction += (prediction==labels).sum().item()\n",
    "            total_prediction += prediction.shape[0]\n",
    "\n",
    "\n",
    "        model.eval()\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data in val_dl:\n",
    "                val_inputs, val_labels = data[0].to(device), data[1].to(device)\n",
    "                # val_inputs, val_labels = data[0], data[1]\n",
    "                val_inputs = val_inputs.float()\n",
    "\n",
    "                #normalization (may be deleted)\n",
    "                # inputs_m, inputs_s = val_inputs.mean(), val_inputs.std()\n",
    "                # val_inputs = (val_inputs - inputs_m) / inputs_s\n",
    "                \n",
    "                # #min max normalization\n",
    "                # inputs_max, inputs_min = val_inputs.max(), val_inputs.min()\n",
    "                # val_inputs = (val_inputs - inputs_min)/(inputs_max-inputs_min)\n",
    "\n",
    "                val_outputs = model(val_inputs)\n",
    "\n",
    "                loss_val = criterion(val_outputs, val_labels)\n",
    "                validation_loss += loss_val.item()\n",
    "\n",
    "                _, prediction = torch.max(val_outputs, 1)\n",
    "                \n",
    "                val_correct_prediction += (prediction==val_labels).sum().item()\n",
    "                val_total_prediction += prediction.shape[0]\n",
    "                \n",
    "                for j in range(len(val_labels)):\n",
    "                    if val_labels[j] == 0:\n",
    "                        total[0] += 1\n",
    "                        if prediction[j] == 0:\n",
    "                            correct[0] += 1\n",
    "                    elif val_labels[j] == 1:\n",
    "                        total[1] += 1\n",
    "                        if prediction[j] == 1:\n",
    "                            correct[1] += 1\n",
    "                    elif val_labels[j] == 2:\n",
    "                        total[2] += 1\n",
    "                        if prediction[j] == 2:\n",
    "                            correct[2] += 1\n",
    "                    elif val_labels[j] == 3:\n",
    "                        total[3] += 1\n",
    "                        if prediction[j] == 3:\n",
    "                            correct[3] += 1\n",
    "\n",
    "        #predicting stats at the end of every epoch\n",
    "        num_batches = len(train_dl)\n",
    "        num_val_batches = len(val_dl)\n",
    "        avg_loss = training_loss / num_batches\n",
    "        val_loss = validation_loss / num_val_batches\n",
    "        train_losses.append(avg_loss)\n",
    "        test_losses.append(val_loss)\n",
    "\n",
    "        accuracy = correct_prediction/total_prediction\n",
    "        val_accuracy = val_correct_prediction/val_total_prediction\n",
    "        sensitivity = (correct[1]+correct[2]+correct[3]) / (total[1]+total[2]+total[3])\n",
    "        specificity = correct[0] / total[0]\n",
    "        icbhi_score = (sensitivity + specificity) / 2\n",
    "        print(total)\n",
    "        print(correct)\n",
    "        print(f'Epoch: {epoch+1}, Loss: {avg_loss:.3f}, Validation Loss: {val_loss:.3f}, Training Accuracy: {accuracy:.3f}, Validation Accuracy{val_accuracy:.3f}, Sensitivity{sensitivity:.3f}, Specificity{specificity:.3f}, ICBHI 2017 Score: {icbhi_score:.3f}')\n",
    "\n",
    "        if icbhi_score > max_icbhi:\n",
    "            max_icbhi = icbhi_score\n",
    "            torch.save(model.state_dict(), \"/mnt/c/Users/papch/OneDrive/Υπολογιστής/Σχολή/Internship-Thesis/model_to_quantize__16092022.pth\")\n",
    "\n",
    "    \n",
    "    train_losses = np.array(train_losses)\n",
    "    test_losses = np.array(test_losses)\n",
    "\n",
    "    # np.save(\"train_losses.npy\", train_losses)\n",
    "    # np.save(\"test_losses.npy\", test_losses)\n",
    "\n",
    "    print('Finished Training')\n",
    "    return train_losses, test_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating the inference function, similar to the training one, will be used just to get results once\n",
    "def inference(model, test_dl):\n",
    "    import warnings\n",
    "    warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "    model.eval()\n",
    "    predictions = []\n",
    "    total_labels = []\n",
    "\n",
    "    correct = [0] * 4\n",
    "    total = [0] * 4\n",
    "    correct_predictions = 0\n",
    "    total_predictions = 0\n",
    "\n",
    "    # start = time.time()\n",
    "    with torch.no_grad():\n",
    "        for data in test_dl:\n",
    "            inputs, labels = data[0].to(device), data[1].to(device)\n",
    "            inputs = inputs.float()\n",
    "            total_labels.extend(labels)\n",
    "\n",
    "            # inputs_m, inputs_s = inputs.mean(), inputs.std()\n",
    "            # inputs = (inputs - inputs_m) / inputs_s\n",
    "\n",
    "            outputs = model(inputs)\n",
    "\n",
    "            _, prediction = torch.max(outputs, 1)\n",
    "    \n",
    "    # end = time.time()\n",
    "    # print(\"INFERENCE TIME: \", end-start)\n",
    "            predictions.extend(prediction)\n",
    "\n",
    "            correct_predictions += (prediction==labels).sum().item()\n",
    "            total_predictions += prediction.shape[0]\n",
    "\n",
    "            for j in range(len(labels)):\n",
    "                if labels[j] == 0:\n",
    "                    total[0] += 1\n",
    "                    if prediction[j] == 0:\n",
    "                        correct[0] += 1\n",
    "                elif labels[j] == 1:\n",
    "                    total[1] += 1\n",
    "                    if prediction[j] == 1:\n",
    "                        correct[1] += 1\n",
    "                elif labels[j] == 2:\n",
    "                    total[2] += 1\n",
    "                    if prediction[j] == 2:\n",
    "                        correct[2] += 1\n",
    "                elif labels[j] == 3:\n",
    "                    total[3] += 1\n",
    "                    if prediction[j] == 3:\n",
    "                        correct[3] += 1\n",
    "\n",
    "    accuracy = correct_predictions/total_predictions\n",
    "\n",
    "    sensitivity = (correct[1]+correct[2]+correct[3]) / (total[1]+total[2]+total[3])\n",
    "    specificity = correct[0] / total[0]\n",
    "    icbhi_score = (sensitivity + specificity) / 2\n",
    "    \n",
    "    print(total)\n",
    "    print(correct)\n",
    "    print(f'Test Accuracy: {accuracy:.4f}, Sensitivity: {sensitivity:.3f}, Specificity: {specificity:.3f}, ICBHI Score: {icbhi_score:.3f}')\n",
    "    return(total_labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "539it [04:45,  1.89it/s]\n",
      "381it [03:00,  2.11it/s]\n"
     ]
    }
   ],
   "source": [
    "#Creating the training and testing sets using functions already written above\n",
    "train_ds = DS(\"ICBHI_Breath_Cycles_Train.csv\", path, is_train=1, aug_scale=1)\n",
    "test_ds = DS(\"ICBHI_Breath_Cycles_Test.csv\", path, is_train=0, aug_scale=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Failed attempt to create a Temporal Convolutional Network (TCN) \n",
    "from torch.nn.utils import weight_norm\n",
    "\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout=0.2):\n",
    "        super(TemporalBlock, self).__init__()\n",
    "        self.conv1 = weight_norm(nn.Conv2d(n_inputs, n_outputs, (1, kernel_size),\n",
    "                                           stride=stride, padding=0, dilation=dilation))\n",
    "        self.pad = torch.nn.ZeroPad2d((padding, 0, 0, 0))\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.conv2 = weight_norm(nn.Conv2d(n_outputs, n_outputs, (1, kernel_size),\n",
    "                                           stride=stride, padding=0, dilation=dilation))\n",
    "        self.net = nn.Sequential(self.pad, self.conv1, self.relu, self.dropout,\n",
    "                                 self.pad, self.conv2, self.relu, self.dropout)\n",
    "        self.downsample = nn.Conv1d(\n",
    "            n_inputs, n_outputs, 1) if n_inputs != n_outputs else None\n",
    "        self.relu = nn.ReLU()\n",
    "        self.init_weights()\n",
    "\n",
    "    def init_weights(self):\n",
    "        self.conv1.weight.data.normal_(0, 0.01)\n",
    "        self.conv2.weight.data.normal_(0, 0.01)\n",
    "        if self.downsample is not None:\n",
    "            self.downsample.weight.data.normal_(0, 0.01)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x.unsqueeze(2)).squeeze(2)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "\n",
    "class TemporalConvNet(nn.Module):\n",
    "    def __init__(self, num_inputs, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TemporalConvNet, self).__init__()\n",
    "        layers = []\n",
    "        num_levels = len(num_channels)\n",
    "        for i in range(num_levels):\n",
    "            dilation_size = 2 ** i\n",
    "            in_channels = num_inputs if i == 0 else num_channels[i-1]\n",
    "            out_channels = num_channels[i]\n",
    "            layers += [TemporalBlock(in_channels, out_channels, kernel_size, stride=1, dilation=dilation_size,\n",
    "                                     padding=(kernel_size-1) * dilation_size, dropout=dropout)]\n",
    "\n",
    "        self.network = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.network(x)\n",
    "\n",
    "class TCNModel(nn.Module):\n",
    "    def __init__(self, num_channels, kernel_size=2, dropout=0.2):\n",
    "        super(TCNModel, self).__init__()\n",
    "        self.tcn = TemporalConvNet(\n",
    "            1, num_channels, kernel_size=kernel_size, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.decoder = nn.Linear(num_channels[-1], 4)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.decoder(self.dropout(self.tcn(x)[:, :, -1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creating the dataloaders\n",
    "train_dl = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True)\n",
    "# val_dl = torch.utils.data.DataLoader(val_ds, batch_size=128, shuffle=False)\n",
    "test_dl = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Initializing the model and choosing the correct device (CPU or GPU)\n",
    "model = Combine2()\n",
    "# model = TCNModel([20] * 3, 3)\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.to(device)\n",
    "next(model.parameters()).device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1579, 649, 388, 145]\n",
      "[631, 350, 120, 7]\n",
      "Epoch: 1, Loss: 1.228, Validation Loss: 1.330, Training Accuracy: 0.438, Validation Accuracy0.401, Sensitivity0.404, Specificity0.400, ICBHI 2017 Score: 0.402\n",
      "[1579, 649, 388, 145]\n",
      "[494, 475, 116, 23]\n",
      "Epoch: 2, Loss: 1.034, Validation Loss: 1.316, Training Accuracy: 0.541, Validation Accuracy0.401, Sensitivity0.519, Specificity0.313, ICBHI 2017 Score: 0.416\n",
      "[1579, 649, 388, 145]\n",
      "[812, 281, 139, 17]\n",
      "Epoch: 3, Loss: 0.969, Validation Loss: 1.283, Training Accuracy: 0.573, Validation Accuracy0.452, Sensitivity0.370, Specificity0.514, ICBHI 2017 Score: 0.442\n",
      "[1579, 649, 388, 145]\n",
      "[880, 358, 101, 18]\n",
      "Epoch: 4, Loss: 0.931, Validation Loss: 1.175, Training Accuracy: 0.596, Validation Accuracy0.491, Sensitivity0.404, Specificity0.557, ICBHI 2017 Score: 0.480\n",
      "[1579, 649, 388, 145]\n",
      "[432, 387, 143, 56]\n",
      "Epoch: 5, Loss: 0.891, Validation Loss: 1.667, Training Accuracy: 0.610, Validation Accuracy0.369, Sensitivity0.496, Specificity0.274, ICBHI 2017 Score: 0.385\n",
      "[1579, 649, 388, 145]\n",
      "[987, 282, 83, 26]\n",
      "Epoch: 6, Loss: 0.871, Validation Loss: 1.160, Training Accuracy: 0.624, Validation Accuracy0.499, Sensitivity0.331, Specificity0.625, ICBHI 2017 Score: 0.478\n",
      "[1579, 649, 388, 145]\n",
      "[735, 291, 116, 41]\n",
      "Epoch: 7, Loss: 0.846, Validation Loss: 1.341, Training Accuracy: 0.642, Validation Accuracy0.428, Sensitivity0.379, Specificity0.465, ICBHI 2017 Score: 0.422\n",
      "[1579, 649, 388, 145]\n",
      "[673, 253, 111, 52]\n",
      "Epoch: 8, Loss: 0.832, Validation Loss: 1.359, Training Accuracy: 0.643, Validation Accuracy0.394, Sensitivity0.352, Specificity0.426, ICBHI 2017 Score: 0.389\n",
      "[1579, 649, 388, 145]\n",
      "[1017, 218, 125, 26]\n",
      "Epoch: 9, Loss: 0.812, Validation Loss: 1.235, Training Accuracy: 0.657, Validation Accuracy0.502, Sensitivity0.312, Specificity0.644, ICBHI 2017 Score: 0.478\n",
      "[1579, 649, 388, 145]\n",
      "[925, 192, 97, 42]\n",
      "Epoch: 10, Loss: 0.803, Validation Loss: 1.351, Training Accuracy: 0.656, Validation Accuracy0.455, Sensitivity0.280, Specificity0.586, ICBHI 2017 Score: 0.433\n",
      "[1579, 649, 388, 145]\n",
      "[778, 425, 74, 30]\n",
      "Epoch: 11, Loss: 0.783, Validation Loss: 1.279, Training Accuracy: 0.669, Validation Accuracy0.473, Sensitivity0.448, Specificity0.493, ICBHI 2017 Score: 0.470\n",
      "[1579, 649, 388, 145]\n",
      "[848, 231, 144, 29]\n",
      "Epoch: 12, Loss: 0.786, Validation Loss: 1.330, Training Accuracy: 0.662, Validation Accuracy0.453, Sensitivity0.342, Specificity0.537, ICBHI 2017 Score: 0.439\n",
      "[1579, 649, 388, 145]\n",
      "[1018, 218, 133, 19]\n",
      "Epoch: 13, Loss: 0.765, Validation Loss: 1.232, Training Accuracy: 0.679, Validation Accuracy0.503, Sensitivity0.313, Specificity0.645, ICBHI 2017 Score: 0.479\n",
      "[1579, 649, 388, 145]\n",
      "[766, 458, 65, 26]\n",
      "Epoch: 14, Loss: 0.749, Validation Loss: 1.370, Training Accuracy: 0.690, Validation Accuracy0.476, Sensitivity0.464, Specificity0.485, ICBHI 2017 Score: 0.475\n",
      "[1579, 649, 388, 145]\n",
      "[972, 313, 67, 13]\n",
      "Epoch: 15, Loss: 0.752, Validation Loss: 1.259, Training Accuracy: 0.686, Validation Accuracy0.494, Sensitivity0.332, Specificity0.616, ICBHI 2017 Score: 0.474\n",
      "[1579, 649, 388, 145]\n",
      "[1002, 216, 100, 22]\n",
      "Epoch: 16, Loss: 0.741, Validation Loss: 1.315, Training Accuracy: 0.687, Validation Accuracy0.485, Sensitivity0.286, Specificity0.635, ICBHI 2017 Score: 0.460\n",
      "[1579, 649, 388, 145]\n",
      "[973, 292, 83, 34]\n",
      "Epoch: 17, Loss: 0.723, Validation Loss: 1.273, Training Accuracy: 0.699, Validation Accuracy0.501, Sensitivity0.346, Specificity0.616, ICBHI 2017 Score: 0.481\n",
      "[1579, 649, 388, 145]\n",
      "[788, 366, 93, 22]\n",
      "Epoch: 18, Loss: 0.715, Validation Loss: 1.304, Training Accuracy: 0.706, Validation Accuracy0.460, Sensitivity0.407, Specificity0.499, ICBHI 2017 Score: 0.453\n",
      "[1579, 649, 388, 145]\n",
      "[968, 339, 88, 11]\n",
      "Epoch: 19, Loss: 0.710, Validation Loss: 1.236, Training Accuracy: 0.707, Validation Accuracy0.509, Sensitivity0.371, Specificity0.613, ICBHI 2017 Score: 0.492\n",
      "[1579, 649, 388, 145]\n",
      "[801, 267, 121, 34]\n",
      "Epoch: 20, Loss: 0.699, Validation Loss: 1.485, Training Accuracy: 0.711, Validation Accuracy0.443, Sensitivity0.357, Specificity0.507, ICBHI 2017 Score: 0.432\n",
      "[1579, 649, 388, 145]\n",
      "[953, 316, 97, 18]\n",
      "Epoch: 21, Loss: 0.690, Validation Loss: 1.239, Training Accuracy: 0.721, Validation Accuracy0.501, Sensitivity0.365, Specificity0.604, ICBHI 2017 Score: 0.484\n",
      "[1579, 649, 388, 145]\n",
      "[631, 286, 165, 48]\n",
      "Epoch: 22, Loss: 0.691, Validation Loss: 1.605, Training Accuracy: 0.718, Validation Accuracy0.409, Sensitivity0.422, Specificity0.400, ICBHI 2017 Score: 0.411\n",
      "[1579, 649, 388, 145]\n",
      "[776, 325, 110, 28]\n",
      "Epoch: 23, Loss: 0.676, Validation Loss: 1.423, Training Accuracy: 0.722, Validation Accuracy0.449, Sensitivity0.392, Specificity0.491, ICBHI 2017 Score: 0.442\n",
      "[1579, 649, 388, 145]\n",
      "[746, 294, 138, 40]\n",
      "Epoch: 24, Loss: 0.671, Validation Loss: 1.481, Training Accuracy: 0.719, Validation Accuracy0.441, Sensitivity0.399, Specificity0.472, ICBHI 2017 Score: 0.436\n",
      "[1579, 649, 388, 145]\n",
      "[786, 308, 119, 45]\n",
      "Epoch: 25, Loss: 0.659, Validation Loss: 1.560, Training Accuracy: 0.736, Validation Accuracy0.456, Sensitivity0.399, Specificity0.498, ICBHI 2017 Score: 0.449\n",
      "[1579, 649, 388, 145]\n",
      "[973, 140, 147, 45]\n",
      "Epoch: 26, Loss: 0.651, Validation Loss: 1.425, Training Accuracy: 0.729, Validation Accuracy0.473, Sensitivity0.281, Specificity0.616, ICBHI 2017 Score: 0.449\n",
      "[1579, 649, 388, 145]\n",
      "[1232, 67, 115, 8]\n",
      "Epoch: 27, Loss: 0.657, Validation Loss: 1.454, Training Accuracy: 0.731, Validation Accuracy0.515, Sensitivity0.161, Specificity0.780, ICBHI 2017 Score: 0.470\n",
      "[1579, 649, 388, 145]\n",
      "[729, 272, 148, 33]\n",
      "Epoch: 28, Loss: 0.653, Validation Loss: 1.546, Training Accuracy: 0.734, Validation Accuracy0.428, Sensitivity0.383, Specificity0.462, ICBHI 2017 Score: 0.422\n",
      "[1579, 649, 388, 145]\n",
      "[864, 214, 127, 35]\n",
      "Epoch: 29, Loss: 0.636, Validation Loss: 1.484, Training Accuracy: 0.738, Validation Accuracy0.449, Sensitivity0.318, Specificity0.547, ICBHI 2017 Score: 0.433\n",
      "[1579, 649, 388, 145]\n",
      "[938, 326, 66, 12]\n",
      "Epoch: 30, Loss: 0.639, Validation Loss: 1.311, Training Accuracy: 0.738, Validation Accuracy0.486, Sensitivity0.342, Specificity0.594, ICBHI 2017 Score: 0.468\n",
      "[1579, 649, 388, 145]\n",
      "[921, 280, 111, 28]\n",
      "Epoch: 31, Loss: 0.626, Validation Loss: 1.297, Training Accuracy: 0.743, Validation Accuracy0.485, Sensitivity0.354, Specificity0.583, ICBHI 2017 Score: 0.469\n",
      "[1579, 649, 388, 145]\n",
      "[708, 345, 126, 37]\n",
      "Epoch: 32, Loss: 0.623, Validation Loss: 1.571, Training Accuracy: 0.747, Validation Accuracy0.440, Sensitivity0.430, Specificity0.448, ICBHI 2017 Score: 0.439\n",
      "[1579, 649, 388, 145]\n",
      "[799, 340, 116, 42]\n",
      "Epoch: 33, Loss: 0.615, Validation Loss: 1.463, Training Accuracy: 0.748, Validation Accuracy0.470, Sensitivity0.421, Specificity0.506, ICBHI 2017 Score: 0.464\n",
      "[1579, 649, 388, 145]\n",
      "[859, 263, 127, 39]\n",
      "Epoch: 34, Loss: 0.610, Validation Loss: 1.502, Training Accuracy: 0.751, Validation Accuracy0.466, Sensitivity0.363, Specificity0.544, ICBHI 2017 Score: 0.453\n",
      "[1579, 649, 388, 145]\n",
      "[1123, 126, 118, 28]\n",
      "Epoch: 35, Loss: 0.616, Validation Loss: 1.411, Training Accuracy: 0.751, Validation Accuracy0.505, Sensitivity0.230, Specificity0.711, ICBHI 2017 Score: 0.471\n",
      "[1579, 649, 388, 145]\n",
      "[1010, 288, 79, 18]\n",
      "Epoch: 36, Loss: 0.609, Validation Loss: 1.266, Training Accuracy: 0.752, Validation Accuracy0.505, Sensitivity0.326, Specificity0.640, ICBHI 2017 Score: 0.483\n",
      "[1579, 649, 388, 145]\n",
      "[866, 346, 101, 29]\n",
      "Epoch: 37, Loss: 0.605, Validation Loss: 1.391, Training Accuracy: 0.759, Validation Accuracy0.486, Sensitivity0.403, Specificity0.548, ICBHI 2017 Score: 0.476\n",
      "[1579, 649, 388, 145]\n",
      "[740, 255, 133, 47]\n",
      "Epoch: 38, Loss: 0.599, Validation Loss: 1.526, Training Accuracy: 0.754, Validation Accuracy0.426, Sensitivity0.368, Specificity0.469, ICBHI 2017 Score: 0.418\n",
      "[1579, 649, 388, 145]\n",
      "[876, 333, 74, 25]\n",
      "Epoch: 39, Loss: 0.587, Validation Loss: 1.525, Training Accuracy: 0.761, Validation Accuracy0.474, Sensitivity0.365, Specificity0.555, ICBHI 2017 Score: 0.460\n",
      "[1579, 649, 388, 145]\n",
      "[635, 329, 137, 50]\n",
      "Epoch: 40, Loss: 0.602, Validation Loss: 1.693, Training Accuracy: 0.758, Validation Accuracy0.417, Sensitivity0.437, Specificity0.402, ICBHI 2017 Score: 0.419\n",
      "[1579, 649, 388, 145]\n",
      "[979, 298, 75, 28]\n",
      "Epoch: 41, Loss: 0.598, Validation Loss: 1.353, Training Accuracy: 0.760, Validation Accuracy0.500, Sensitivity0.339, Specificity0.620, ICBHI 2017 Score: 0.480\n",
      "[1579, 649, 388, 145]\n",
      "[1054, 239, 98, 30]\n",
      "Epoch: 42, Loss: 0.582, Validation Loss: 1.370, Training Accuracy: 0.765, Validation Accuracy0.515, Sensitivity0.310, Specificity0.668, ICBHI 2017 Score: 0.489\n",
      "[1579, 649, 388, 145]\n",
      "[858, 329, 83, 39]\n",
      "Epoch: 43, Loss: 0.580, Validation Loss: 1.424, Training Accuracy: 0.767, Validation Accuracy0.474, Sensitivity0.382, Specificity0.543, ICBHI 2017 Score: 0.462\n",
      "[1579, 649, 388, 145]\n",
      "[948, 236, 114, 30]\n",
      "Epoch: 44, Loss: 0.565, Validation Loss: 1.446, Training Accuracy: 0.771, Validation Accuracy0.481, Sensitivity0.321, Specificity0.600, ICBHI 2017 Score: 0.461\n",
      "[1579, 649, 388, 145]\n",
      "[620, 367, 97, 35]\n",
      "Epoch: 45, Loss: 0.554, Validation Loss: 1.646, Training Accuracy: 0.779, Validation Accuracy0.405, Sensitivity0.422, Specificity0.393, ICBHI 2017 Score: 0.407\n",
      "[1579, 649, 388, 145]\n",
      "[966, 207, 131, 28]\n",
      "Epoch: 46, Loss: 0.566, Validation Loss: 1.439, Training Accuracy: 0.774, Validation Accuracy0.482, Sensitivity0.310, Specificity0.612, ICBHI 2017 Score: 0.461\n",
      "[1579, 649, 388, 145]\n",
      "[1014, 301, 92, 23]\n",
      "Epoch: 47, Loss: 0.551, Validation Loss: 1.341, Training Accuracy: 0.779, Validation Accuracy0.518, Sensitivity0.352, Specificity0.642, ICBHI 2017 Score: 0.497\n",
      "[1579, 649, 388, 145]\n",
      "[807, 228, 155, 36]\n",
      "Epoch: 48, Loss: 0.543, Validation Loss: 1.669, Training Accuracy: 0.779, Validation Accuracy0.444, Sensitivity0.354, Specificity0.511, ICBHI 2017 Score: 0.433\n",
      "[1579, 649, 388, 145]\n",
      "[809, 292, 109, 42]\n",
      "Epoch: 49, Loss: 0.551, Validation Loss: 1.606, Training Accuracy: 0.783, Validation Accuracy0.453, Sensitivity0.375, Specificity0.512, ICBHI 2017 Score: 0.444\n",
      "[1579, 649, 388, 145]\n",
      "[698, 275, 120, 56]\n",
      "Epoch: 50, Loss: 0.555, Validation Loss: 1.650, Training Accuracy: 0.775, Validation Accuracy0.416, Sensitivity0.382, Specificity0.442, ICBHI 2017 Score: 0.412\n",
      "[1579, 649, 388, 145]\n",
      "[1011, 268, 94, 22]\n",
      "Epoch: 51, Loss: 0.548, Validation Loss: 1.403, Training Accuracy: 0.780, Validation Accuracy0.505, Sensitivity0.325, Specificity0.640, ICBHI 2017 Score: 0.483\n",
      "[1579, 649, 388, 145]\n",
      "[746, 333, 90, 40]\n",
      "Epoch: 52, Loss: 0.542, Validation Loss: 1.626, Training Accuracy: 0.780, Validation Accuracy0.438, Sensitivity0.392, Specificity0.472, ICBHI 2017 Score: 0.432\n",
      "[1579, 649, 388, 145]\n",
      "[642, 359, 126, 48]\n",
      "Epoch: 53, Loss: 0.539, Validation Loss: 1.661, Training Accuracy: 0.785, Validation Accuracy0.426, Sensitivity0.451, Specificity0.407, ICBHI 2017 Score: 0.429\n",
      "[1579, 649, 388, 145]\n",
      "[913, 285, 97, 23]\n",
      "Epoch: 54, Loss: 0.542, Validation Loss: 1.469, Training Accuracy: 0.786, Validation Accuracy0.477, Sensitivity0.343, Specificity0.578, ICBHI 2017 Score: 0.460\n",
      "[1579, 649, 388, 145]\n",
      "[778, 318, 144, 28]\n",
      "Epoch: 55, Loss: 0.536, Validation Loss: 1.539, Training Accuracy: 0.784, Validation Accuracy0.459, Sensitivity0.415, Specificity0.493, ICBHI 2017 Score: 0.454\n",
      "[1579, 649, 388, 145]\n",
      "[992, 318, 94, 26]\n",
      "Epoch: 56, Loss: 0.540, Validation Loss: 1.369, Training Accuracy: 0.784, Validation Accuracy0.518, Sensitivity0.371, Specificity0.628, ICBHI 2017 Score: 0.499\n",
      "[1579, 649, 388, 145]\n",
      "[674, 328, 99, 50]\n",
      "Epoch: 57, Loss: 0.535, Validation Loss: 1.735, Training Accuracy: 0.783, Validation Accuracy0.417, Sensitivity0.404, Specificity0.427, ICBHI 2017 Score: 0.415\n",
      "[1579, 649, 388, 145]\n",
      "[1004, 318, 82, 10]\n",
      "Epoch: 58, Loss: 0.523, Validation Loss: 1.365, Training Accuracy: 0.790, Validation Accuracy0.512, Sensitivity0.347, Specificity0.636, ICBHI 2017 Score: 0.491\n",
      "[1579, 649, 388, 145]\n",
      "[997, 340, 73, 16]\n",
      "Epoch: 59, Loss: 0.527, Validation Loss: 1.395, Training Accuracy: 0.791, Validation Accuracy0.516, Sensitivity0.363, Specificity0.631, ICBHI 2017 Score: 0.497\n",
      "[1579, 649, 388, 145]\n",
      "[885, 279, 112, 41]\n",
      "Epoch: 60, Loss: 0.531, Validation Loss: 1.493, Training Accuracy: 0.786, Validation Accuracy0.477, Sensitivity0.365, Specificity0.560, ICBHI 2017 Score: 0.463\n",
      "[1579, 649, 388, 145]\n",
      "[867, 316, 127, 33]\n",
      "Epoch: 61, Loss: 0.520, Validation Loss: 1.423, Training Accuracy: 0.794, Validation Accuracy0.486, Sensitivity0.403, Specificity0.549, ICBHI 2017 Score: 0.476\n",
      "[1579, 649, 388, 145]\n",
      "[1019, 334, 83, 16]\n",
      "Epoch: 62, Loss: 0.508, Validation Loss: 1.393, Training Accuracy: 0.799, Validation Accuracy0.526, Sensitivity0.366, Specificity0.645, ICBHI 2017 Score: 0.506\n",
      "[1579, 649, 388, 145]\n",
      "[963, 125, 160, 25]\n",
      "Epoch: 63, Loss: 0.506, Validation Loss: 1.671, Training Accuracy: 0.797, Validation Accuracy0.461, Sensitivity0.262, Specificity0.610, ICBHI 2017 Score: 0.436\n",
      "[1579, 649, 388, 145]\n",
      "[1054, 108, 171, 21]\n",
      "Epoch: 64, Loss: 0.532, Validation Loss: 1.640, Training Accuracy: 0.785, Validation Accuracy0.490, Sensitivity0.254, Specificity0.668, ICBHI 2017 Score: 0.461\n",
      "[1579, 649, 388, 145]\n",
      "[790, 307, 113, 48]\n",
      "Epoch: 65, Loss: 0.522, Validation Loss: 1.559, Training Accuracy: 0.791, Validation Accuracy0.456, Sensitivity0.396, Specificity0.500, ICBHI 2017 Score: 0.448\n",
      "[1579, 649, 388, 145]\n",
      "[899, 333, 102, 32]\n",
      "Epoch: 66, Loss: 0.507, Validation Loss: 1.441, Training Accuracy: 0.795, Validation Accuracy0.495, Sensitivity0.395, Specificity0.569, ICBHI 2017 Score: 0.482\n",
      "[1579, 649, 388, 145]\n",
      "[1016, 243, 104, 16]\n",
      "Epoch: 67, Loss: 0.511, Validation Loss: 1.463, Training Accuracy: 0.799, Validation Accuracy0.499, Sensitivity0.307, Specificity0.643, ICBHI 2017 Score: 0.475\n",
      "[1579, 649, 388, 145]\n",
      "[1008, 289, 102, 13]\n",
      "Epoch: 68, Loss: 0.505, Validation Loss: 1.426, Training Accuracy: 0.799, Validation Accuracy0.511, Sensitivity0.342, Specificity0.638, ICBHI 2017 Score: 0.490\n",
      "[1579, 649, 388, 145]\n",
      "[773, 345, 108, 30]\n",
      "Epoch: 69, Loss: 0.514, Validation Loss: 1.652, Training Accuracy: 0.796, Validation Accuracy0.455, Sensitivity0.409, Specificity0.490, ICBHI 2017 Score: 0.449\n",
      "[1579, 649, 388, 145]\n",
      "[830, 308, 113, 33]\n",
      "Epoch: 70, Loss: 0.497, Validation Loss: 1.554, Training Accuracy: 0.800, Validation Accuracy0.465, Sensitivity0.384, Specificity0.526, ICBHI 2017 Score: 0.455\n",
      "[1579, 649, 388, 145]\n",
      "[922, 146, 152, 27]\n",
      "Epoch: 71, Loss: 0.504, Validation Loss: 1.620, Training Accuracy: 0.802, Validation Accuracy0.452, Sensitivity0.275, Specificity0.584, ICBHI 2017 Score: 0.429\n",
      "[1579, 649, 388, 145]\n",
      "[919, 199, 124, 26]\n",
      "Epoch: 72, Loss: 0.494, Validation Loss: 1.643, Training Accuracy: 0.804, Validation Accuracy0.459, Sensitivity0.295, Specificity0.582, ICBHI 2017 Score: 0.439\n",
      "[1579, 649, 388, 145]\n",
      "[860, 301, 117, 25]\n",
      "Epoch: 73, Loss: 0.510, Validation Loss: 1.514, Training Accuracy: 0.796, Validation Accuracy0.472, Sensitivity0.375, Specificity0.545, ICBHI 2017 Score: 0.460\n",
      "[1579, 649, 388, 145]\n",
      "[861, 277, 111, 27]\n",
      "Epoch: 74, Loss: 0.488, Validation Loss: 1.673, Training Accuracy: 0.807, Validation Accuracy0.462, Sensitivity0.351, Specificity0.545, ICBHI 2017 Score: 0.448\n",
      "[1579, 649, 388, 145]\n",
      "[963, 129, 174, 14]\n",
      "Epoch: 75, Loss: 0.496, Validation Loss: 1.790, Training Accuracy: 0.801, Validation Accuracy0.464, Sensitivity0.268, Specificity0.610, ICBHI 2017 Score: 0.439\n",
      "[1579, 649, 388, 145]\n",
      "[906, 235, 136, 35]\n",
      "Epoch: 76, Loss: 0.483, Validation Loss: 1.718, Training Accuracy: 0.808, Validation Accuracy0.475, Sensitivity0.343, Specificity0.574, ICBHI 2017 Score: 0.459\n",
      "[1579, 649, 388, 145]\n",
      "[787, 331, 95, 20]\n",
      "Epoch: 77, Loss: 0.490, Validation Loss: 1.682, Training Accuracy: 0.806, Validation Accuracy0.447, Sensitivity0.377, Specificity0.498, ICBHI 2017 Score: 0.438\n",
      "[1579, 649, 388, 145]\n",
      "[916, 363, 51, 19]\n",
      "Epoch: 78, Loss: 0.494, Validation Loss: 1.545, Training Accuracy: 0.802, Validation Accuracy0.489, Sensitivity0.366, Specificity0.580, ICBHI 2017 Score: 0.473\n",
      "[1579, 649, 388, 145]\n",
      "[873, 360, 79, 27]\n",
      "Epoch: 79, Loss: 0.483, Validation Loss: 1.543, Training Accuracy: 0.807, Validation Accuracy0.485, Sensitivity0.394, Specificity0.553, ICBHI 2017 Score: 0.474\n",
      "[1579, 649, 388, 145]\n",
      "[838, 281, 120, 31]\n",
      "Epoch: 80, Loss: 0.479, Validation Loss: 1.737, Training Accuracy: 0.814, Validation Accuracy0.460, Sensitivity0.365, Specificity0.531, ICBHI 2017 Score: 0.448\n",
      "[1579, 649, 388, 145]\n",
      "[845, 284, 117, 38]\n",
      "Epoch: 81, Loss: 0.492, Validation Loss: 1.624, Training Accuracy: 0.804, Validation Accuracy0.465, Sensitivity0.371, Specificity0.535, ICBHI 2017 Score: 0.453\n",
      "[1579, 649, 388, 145]\n",
      "[929, 296, 109, 28]\n",
      "Epoch: 82, Loss: 0.469, Validation Loss: 1.529, Training Accuracy: 0.814, Validation Accuracy0.493, Sensitivity0.366, Specificity0.588, ICBHI 2017 Score: 0.477\n",
      "[1579, 649, 388, 145]\n",
      "[986, 300, 90, 21]\n",
      "Epoch: 83, Loss: 0.484, Validation Loss: 1.490, Training Accuracy: 0.808, Validation Accuracy0.506, Sensitivity0.348, Specificity0.624, ICBHI 2017 Score: 0.486\n",
      "[1579, 649, 388, 145]\n",
      "[838, 246, 147, 25]\n",
      "Epoch: 84, Loss: 0.483, Validation Loss: 1.717, Training Accuracy: 0.812, Validation Accuracy0.455, Sensitivity0.354, Specificity0.531, ICBHI 2017 Score: 0.442\n",
      "[1579, 649, 388, 145]\n",
      "[815, 356, 102, 37]\n",
      "Epoch: 85, Loss: 0.473, Validation Loss: 1.651, Training Accuracy: 0.810, Validation Accuracy0.474, Sensitivity0.419, Specificity0.516, ICBHI 2017 Score: 0.467\n",
      "[1579, 649, 388, 145]\n",
      "[913, 290, 93, 26]\n",
      "Epoch: 86, Loss: 0.476, Validation Loss: 1.624, Training Accuracy: 0.808, Validation Accuracy0.479, Sensitivity0.346, Specificity0.578, ICBHI 2017 Score: 0.462\n",
      "[1579, 649, 388, 145]\n",
      "[1010, 271, 79, 22]\n",
      "Epoch: 87, Loss: 0.480, Validation Loss: 1.474, Training Accuracy: 0.811, Validation Accuracy0.501, Sensitivity0.315, Specificity0.640, ICBHI 2017 Score: 0.477\n",
      "[1579, 649, 388, 145]\n",
      "[847, 275, 108, 42]\n",
      "Epoch: 88, Loss: 0.460, Validation Loss: 1.646, Training Accuracy: 0.820, Validation Accuracy0.461, Sensitivity0.360, Specificity0.536, ICBHI 2017 Score: 0.448\n",
      "[1579, 649, 388, 145]\n",
      "[798, 246, 130, 43]\n",
      "Epoch: 89, Loss: 0.478, Validation Loss: 1.747, Training Accuracy: 0.809, Validation Accuracy0.441, Sensitivity0.354, Specificity0.505, ICBHI 2017 Score: 0.430\n",
      "[1579, 649, 388, 145]\n",
      "[948, 305, 99, 34]\n",
      "Epoch: 90, Loss: 0.483, Validation Loss: 1.521, Training Accuracy: 0.811, Validation Accuracy0.502, Sensitivity0.371, Specificity0.600, ICBHI 2017 Score: 0.485\n",
      "[1579, 649, 388, 145]\n",
      "[902, 267, 104, 14]\n",
      "Epoch: 91, Loss: 0.478, Validation Loss: 1.631, Training Accuracy: 0.808, Validation Accuracy0.466, Sensitivity0.326, Specificity0.571, ICBHI 2017 Score: 0.448\n",
      "[1579, 649, 388, 145]\n",
      "[1072, 162, 120, 16]\n",
      "Epoch: 92, Loss: 0.464, Validation Loss: 1.580, Training Accuracy: 0.818, Validation Accuracy0.496, Sensitivity0.252, Specificity0.679, ICBHI 2017 Score: 0.466\n",
      "[1579, 649, 388, 145]\n",
      "[700, 292, 116, 32]\n",
      "Epoch: 93, Loss: 0.469, Validation Loss: 1.843, Training Accuracy: 0.816, Validation Accuracy0.413, Sensitivity0.372, Specificity0.443, ICBHI 2017 Score: 0.408\n",
      "[1579, 649, 388, 145]\n",
      "[865, 294, 90, 33]\n",
      "Epoch: 94, Loss: 0.468, Validation Loss: 1.670, Training Accuracy: 0.814, Validation Accuracy0.464, Sensitivity0.353, Specificity0.548, ICBHI 2017 Score: 0.450\n",
      "[1579, 649, 388, 145]\n",
      "[1196, 240, 56, 15]\n",
      "Epoch: 95, Loss: 0.462, Validation Loss: 1.490, Training Accuracy: 0.816, Validation Accuracy0.546, Sensitivity0.263, Specificity0.757, ICBHI 2017 Score: 0.510\n",
      "[1579, 649, 388, 145]\n",
      "[691, 342, 120, 50]\n",
      "Epoch: 96, Loss: 0.460, Validation Loss: 1.764, Training Accuracy: 0.818, Validation Accuracy0.436, Sensitivity0.433, Specificity0.438, ICBHI 2017 Score: 0.435\n",
      "[1579, 649, 388, 145]\n",
      "[723, 338, 127, 46]\n",
      "Epoch: 97, Loss: 0.449, Validation Loss: 1.746, Training Accuracy: 0.823, Validation Accuracy0.447, Sensitivity0.432, Specificity0.458, ICBHI 2017 Score: 0.445\n",
      "[1579, 649, 388, 145]\n",
      "[718, 330, 124, 40]\n",
      "Epoch: 98, Loss: 0.456, Validation Loss: 1.797, Training Accuracy: 0.821, Validation Accuracy0.439, Sensitivity0.418, Specificity0.455, ICBHI 2017 Score: 0.436\n",
      "[1579, 649, 388, 145]\n",
      "[958, 237, 119, 24]\n",
      "Epoch: 99, Loss: 0.471, Validation Loss: 1.578, Training Accuracy: 0.816, Validation Accuracy0.485, Sensitivity0.321, Specificity0.607, ICBHI 2017 Score: 0.464\n",
      "[1579, 649, 388, 145]\n",
      "[972, 278, 94, 16]\n",
      "Epoch: 100, Loss: 0.443, Validation Loss: 1.620, Training Accuracy: 0.823, Validation Accuracy0.493, Sensitivity0.328, Specificity0.616, ICBHI 2017 Score: 0.472\n",
      "[1579, 649, 388, 145]\n",
      "[1157, 242, 65, 23]\n",
      "Epoch: 101, Loss: 0.452, Validation Loss: 1.469, Training Accuracy: 0.819, Validation Accuracy0.539, Sensitivity0.279, Specificity0.733, ICBHI 2017 Score: 0.506\n",
      "[1579, 649, 388, 145]\n",
      "[732, 322, 111, 40]\n",
      "Epoch: 102, Loss: 0.453, Validation Loss: 1.760, Training Accuracy: 0.821, Validation Accuracy0.436, Sensitivity0.400, Specificity0.464, ICBHI 2017 Score: 0.432\n",
      "[1579, 649, 388, 145]\n",
      "[836, 284, 108, 46]\n",
      "Epoch: 103, Loss: 0.455, Validation Loss: 1.722, Training Accuracy: 0.817, Validation Accuracy0.461, Sensitivity0.371, Specificity0.529, ICBHI 2017 Score: 0.450\n",
      "[1579, 649, 388, 145]\n",
      "[773, 285, 115, 28]\n",
      "Epoch: 104, Loss: 0.447, Validation Loss: 1.808, Training Accuracy: 0.826, Validation Accuracy0.435, Sensitivity0.362, Specificity0.490, ICBHI 2017 Score: 0.426\n",
      "[1579, 649, 388, 145]\n",
      "[931, 282, 91, 26]\n",
      "Epoch: 105, Loss: 0.444, Validation Loss: 1.632, Training Accuracy: 0.823, Validation Accuracy0.482, Sensitivity0.338, Specificity0.590, ICBHI 2017 Score: 0.464\n",
      "[1579, 649, 388, 145]\n",
      "[978, 169, 97, 32]\n",
      "Epoch: 106, Loss: 0.434, Validation Loss: 1.715, Training Accuracy: 0.825, Validation Accuracy0.462, Sensitivity0.252, Specificity0.619, ICBHI 2017 Score: 0.436\n",
      "[1579, 649, 388, 145]\n",
      "[756, 298, 120, 33]\n",
      "Epoch: 107, Loss: 0.447, Validation Loss: 1.890, Training Accuracy: 0.821, Validation Accuracy0.437, Sensitivity0.382, Specificity0.479, ICBHI 2017 Score: 0.430\n",
      "[1579, 649, 388, 145]\n",
      "[927, 370, 68, 29]\n",
      "Epoch: 108, Loss: 0.436, Validation Loss: 1.588, Training Accuracy: 0.830, Validation Accuracy0.505, Sensitivity0.395, Specificity0.587, ICBHI 2017 Score: 0.491\n",
      "[1579, 649, 388, 145]\n",
      "[828, 278, 125, 40]\n",
      "Epoch: 109, Loss: 0.435, Validation Loss: 1.741, Training Accuracy: 0.825, Validation Accuracy0.460, Sensitivity0.375, Specificity0.524, ICBHI 2017 Score: 0.450\n",
      "[1579, 649, 388, 145]\n",
      "[938, 232, 124, 33]\n",
      "Epoch: 110, Loss: 0.432, Validation Loss: 1.749, Training Accuracy: 0.830, Validation Accuracy0.481, Sensitivity0.329, Specificity0.594, ICBHI 2017 Score: 0.462\n",
      "[1579, 649, 388, 145]\n",
      "[1022, 256, 92, 22]\n",
      "Epoch: 111, Loss: 0.438, Validation Loss: 1.548, Training Accuracy: 0.829, Validation Accuracy0.504, Sensitivity0.313, Specificity0.647, ICBHI 2017 Score: 0.480\n",
      "[1579, 649, 388, 145]\n",
      "[876, 304, 93, 25]\n",
      "Epoch: 112, Loss: 0.442, Validation Loss: 1.635, Training Accuracy: 0.823, Validation Accuracy0.470, Sensitivity0.357, Specificity0.555, ICBHI 2017 Score: 0.456\n",
      "[1579, 649, 388, 145]\n",
      "[736, 322, 118, 37]\n",
      "Epoch: 113, Loss: 0.438, Validation Loss: 1.834, Training Accuracy: 0.830, Validation Accuracy0.439, Sensitivity0.404, Specificity0.466, ICBHI 2017 Score: 0.435\n",
      "[1579, 649, 388, 145]\n",
      "[811, 264, 139, 26]\n",
      "Epoch: 114, Loss: 0.429, Validation Loss: 1.915, Training Accuracy: 0.830, Validation Accuracy0.449, Sensitivity0.363, Specificity0.514, ICBHI 2017 Score: 0.438\n",
      "[1579, 649, 388, 145]\n",
      "[903, 286, 88, 27]\n",
      "Epoch: 115, Loss: 0.428, Validation Loss: 1.677, Training Accuracy: 0.829, Validation Accuracy0.472, Sensitivity0.339, Specificity0.572, ICBHI 2017 Score: 0.456\n",
      "[1579, 649, 388, 145]\n",
      "[722, 314, 115, 43]\n",
      "Epoch: 116, Loss: 0.431, Validation Loss: 1.941, Training Accuracy: 0.831, Validation Accuracy0.432, Sensitivity0.399, Specificity0.457, ICBHI 2017 Score: 0.428\n",
      "[1579, 649, 388, 145]\n",
      "[738, 305, 147, 36]\n",
      "Epoch: 117, Loss: 0.433, Validation Loss: 1.951, Training Accuracy: 0.830, Validation Accuracy0.444, Sensitivity0.413, Specificity0.467, ICBHI 2017 Score: 0.440\n",
      "[1579, 649, 388, 145]\n",
      "[909, 313, 109, 22]\n",
      "Epoch: 118, Loss: 0.428, Validation Loss: 1.599, Training Accuracy: 0.832, Validation Accuracy0.490, Sensitivity0.376, Specificity0.576, ICBHI 2017 Score: 0.476\n",
      "[1579, 649, 388, 145]\n",
      "[818, 245, 152, 26]\n",
      "Epoch: 119, Loss: 0.429, Validation Loss: 1.809, Training Accuracy: 0.827, Validation Accuracy0.449, Sensitivity0.358, Specificity0.518, ICBHI 2017 Score: 0.438\n",
      "[1579, 649, 388, 145]\n",
      "[812, 279, 140, 33]\n",
      "Epoch: 120, Loss: 0.430, Validation Loss: 1.756, Training Accuracy: 0.827, Validation Accuracy0.458, Sensitivity0.382, Specificity0.514, ICBHI 2017 Score: 0.448\n",
      "[1579, 649, 388, 145]\n",
      "[738, 319, 136, 26]\n",
      "Epoch: 121, Loss: 0.423, Validation Loss: 1.875, Training Accuracy: 0.832, Validation Accuracy0.442, Sensitivity0.407, Specificity0.467, ICBHI 2017 Score: 0.437\n",
      "[1579, 649, 388, 145]\n",
      "[854, 309, 127, 34]\n",
      "Epoch: 122, Loss: 0.430, Validation Loss: 1.744, Training Accuracy: 0.829, Validation Accuracy0.480, Sensitivity0.398, Specificity0.541, ICBHI 2017 Score: 0.469\n",
      "[1579, 649, 388, 145]\n",
      "[860, 319, 117, 35]\n",
      "Epoch: 123, Loss: 0.428, Validation Loss: 1.751, Training Accuracy: 0.834, Validation Accuracy0.482, Sensitivity0.398, Specificity0.545, ICBHI 2017 Score: 0.472\n",
      "[1579, 649, 388, 145]\n",
      "[891, 215, 136, 29]\n",
      "Epoch: 124, Loss: 0.426, Validation Loss: 1.793, Training Accuracy: 0.829, Validation Accuracy0.460, Sensitivity0.321, Specificity0.564, ICBHI 2017 Score: 0.443\n",
      "[1579, 649, 388, 145]\n",
      "[809, 287, 118, 31]\n",
      "Epoch: 125, Loss: 0.417, Validation Loss: 1.757, Training Accuracy: 0.834, Validation Accuracy0.451, Sensitivity0.369, Specificity0.512, ICBHI 2017 Score: 0.441\n",
      "[1579, 649, 388, 145]\n",
      "[877, 170, 149, 34]\n",
      "Epoch: 126, Loss: 0.409, Validation Loss: 1.837, Training Accuracy: 0.837, Validation Accuracy0.445, Sensitivity0.299, Specificity0.555, ICBHI 2017 Score: 0.427\n",
      "[1579, 649, 388, 145]\n",
      "[918, 246, 109, 40]\n",
      "Epoch: 127, Loss: 0.416, Validation Loss: 1.720, Training Accuracy: 0.837, Validation Accuracy0.476, Sensitivity0.334, Specificity0.581, ICBHI 2017 Score: 0.458\n",
      "[1579, 649, 388, 145]\n",
      "[1176, 177, 99, 17]\n",
      "Epoch: 128, Loss: 0.418, Validation Loss: 1.586, Training Accuracy: 0.835, Validation Accuracy0.532, Sensitivity0.248, Specificity0.745, ICBHI 2017 Score: 0.496\n",
      "[1579, 649, 388, 145]\n",
      "[991, 194, 138, 23]\n",
      "Epoch: 129, Loss: 0.425, Validation Loss: 1.659, Training Accuracy: 0.831, Validation Accuracy0.488, Sensitivity0.300, Specificity0.628, ICBHI 2017 Score: 0.464\n",
      "[1579, 649, 388, 145]\n",
      "[829, 226, 132, 44]\n",
      "Epoch: 130, Loss: 0.420, Validation Loss: 1.805, Training Accuracy: 0.833, Validation Accuracy0.446, Sensitivity0.340, Specificity0.525, ICBHI 2017 Score: 0.433\n",
      "[1579, 649, 388, 145]\n",
      "[874, 272, 114, 29]\n",
      "Epoch: 131, Loss: 0.421, Validation Loss: 1.755, Training Accuracy: 0.835, Validation Accuracy0.467, Sensitivity0.351, Specificity0.554, ICBHI 2017 Score: 0.452\n",
      "[1579, 649, 388, 145]\n",
      "[739, 297, 137, 33]\n",
      "Epoch: 132, Loss: 0.422, Validation Loss: 1.816, Training Accuracy: 0.838, Validation Accuracy0.437, Sensitivity0.395, Specificity0.468, ICBHI 2017 Score: 0.432\n",
      "[1579, 649, 388, 145]\n",
      "[741, 352, 104, 39]\n",
      "Epoch: 133, Loss: 0.420, Validation Loss: 1.793, Training Accuracy: 0.831, Validation Accuracy0.448, Sensitivity0.419, Specificity0.469, ICBHI 2017 Score: 0.444\n",
      "[1579, 649, 388, 145]\n",
      "[1005, 222, 135, 27]\n",
      "Epoch: 134, Loss: 0.410, Validation Loss: 1.611, Training Accuracy: 0.835, Validation Accuracy0.503, Sensitivity0.325, Specificity0.636, ICBHI 2017 Score: 0.481\n",
      "[1579, 649, 388, 145]\n",
      "[967, 280, 113, 33]\n",
      "Epoch: 135, Loss: 0.414, Validation Loss: 1.615, Training Accuracy: 0.834, Validation Accuracy0.505, Sensitivity0.360, Specificity0.612, ICBHI 2017 Score: 0.486\n",
      "[1579, 649, 388, 145]\n",
      "[746, 342, 112, 34]\n",
      "Epoch: 136, Loss: 0.408, Validation Loss: 1.804, Training Accuracy: 0.841, Validation Accuracy0.447, Sensitivity0.413, Specificity0.472, ICBHI 2017 Score: 0.443\n",
      "[1579, 649, 388, 145]\n",
      "[796, 281, 119, 38]\n",
      "Epoch: 137, Loss: 0.413, Validation Loss: 1.833, Training Accuracy: 0.839, Validation Accuracy0.447, Sensitivity0.371, Specificity0.504, ICBHI 2017 Score: 0.437\n",
      "[1579, 649, 388, 145]\n",
      "[863, 292, 112, 31]\n",
      "Epoch: 138, Loss: 0.407, Validation Loss: 1.784, Training Accuracy: 0.840, Validation Accuracy0.470, Sensitivity0.368, Specificity0.547, ICBHI 2017 Score: 0.457\n",
      "[1579, 649, 388, 145]\n",
      "[827, 286, 119, 22]\n",
      "Epoch: 139, Loss: 0.417, Validation Loss: 1.836, Training Accuracy: 0.835, Validation Accuracy0.454, Sensitivity0.361, Specificity0.524, ICBHI 2017 Score: 0.443\n",
      "[1579, 649, 388, 145]\n",
      "[816, 338, 98, 40]\n",
      "Epoch: 140, Loss: 0.406, Validation Loss: 1.747, Training Accuracy: 0.840, Validation Accuracy0.468, Sensitivity0.403, Specificity0.517, ICBHI 2017 Score: 0.460\n",
      "[1579, 649, 388, 145]\n",
      "[886, 290, 104, 28]\n",
      "Epoch: 141, Loss: 0.406, Validation Loss: 1.739, Training Accuracy: 0.841, Validation Accuracy0.474, Sensitivity0.357, Specificity0.561, ICBHI 2017 Score: 0.459\n",
      "[1579, 649, 388, 145]\n",
      "[867, 288, 98, 34]\n",
      "Epoch: 142, Loss: 0.397, Validation Loss: 1.892, Training Accuracy: 0.845, Validation Accuracy0.466, Sensitivity0.355, Specificity0.549, ICBHI 2017 Score: 0.452\n",
      "[1579, 649, 388, 145]\n",
      "[1135, 219, 95, 16]\n",
      "Epoch: 143, Loss: 0.410, Validation Loss: 1.548, Training Accuracy: 0.838, Validation Accuracy0.531, Sensitivity0.279, Specificity0.719, ICBHI 2017 Score: 0.499\n",
      "[1579, 649, 388, 145]\n",
      "[850, 319, 104, 31]\n",
      "Epoch: 144, Loss: 0.398, Validation Loss: 1.754, Training Accuracy: 0.842, Validation Accuracy0.472, Sensitivity0.384, Specificity0.538, ICBHI 2017 Score: 0.461\n",
      "[1579, 649, 388, 145]\n",
      "[745, 319, 138, 43]\n",
      "Epoch: 145, Loss: 0.392, Validation Loss: 1.918, Training Accuracy: 0.848, Validation Accuracy0.451, Sensitivity0.423, Specificity0.472, ICBHI 2017 Score: 0.447\n",
      "[1579, 649, 388, 145]\n",
      "[936, 314, 136, 23]\n",
      "Epoch: 146, Loss: 0.401, Validation Loss: 1.643, Training Accuracy: 0.845, Validation Accuracy0.510, Sensitivity0.400, Specificity0.593, ICBHI 2017 Score: 0.496\n",
      "[1579, 649, 388, 145]\n",
      "[900, 257, 102, 28]\n",
      "Epoch: 147, Loss: 0.385, Validation Loss: 1.845, Training Accuracy: 0.851, Validation Accuracy0.466, Sensitivity0.327, Specificity0.570, ICBHI 2017 Score: 0.449\n",
      "[1579, 649, 388, 145]\n",
      "[837, 233, 141, 25]\n",
      "Epoch: 148, Loss: 0.391, Validation Loss: 1.833, Training Accuracy: 0.849, Validation Accuracy0.448, Sensitivity0.338, Specificity0.530, ICBHI 2017 Score: 0.434\n",
      "[1579, 649, 388, 145]\n",
      "[773, 366, 102, 23]\n",
      "Epoch: 149, Loss: 0.390, Validation Loss: 1.909, Training Accuracy: 0.847, Validation Accuracy0.458, Sensitivity0.415, Specificity0.490, ICBHI 2017 Score: 0.452\n",
      "[1579, 649, 388, 145]\n",
      "[756, 352, 99, 33]\n",
      "Epoch: 150, Loss: 0.393, Validation Loss: 1.852, Training Accuracy: 0.846, Validation Accuracy0.449, Sensitivity0.409, Specificity0.479, ICBHI 2017 Score: 0.444\n",
      "Finished Training\n",
      "MAX ICBHI SCORE IS: 0.51027739289683\n"
     ]
    }
   ],
   "source": [
    "#IF YOU WANT TO TRAIN THE MODEL (NEW)\n",
    "train_losses, test_losses = training(model, train_dl, test_dl, 150)\n",
    "print(\"MAX ICBHI SCORE IS:\", max_icbhi)\n",
    "# inference(model, test_dl)\n",
    "np.save(\"train_losses1.npy\", train_losses, allow_pickle=True)\n",
    "np.save(\"test_losses1.npy\", test_losses, allow_pickle=True)\n",
    "np.save(\"train_losses2.npy\", train_losses, allow_pickle=False)\n",
    "np.save(\"test_losses2.npy\", test_losses, allow_pickle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.save(\"train_losses.npy\", train_losses)\n",
    "# np.save(\"test_losses.npy\", test_losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1579, 649, 388, 145]\n",
      "[1252, 215, 71, 18]\n",
      "Test Accuracy: 0.5636, Sensitivity: 0.257, Specificity: 0.793, ICBHI Score: 0.525\n"
     ]
    }
   ],
   "source": [
    "#If you want to load the already trained model\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "normal = Combine2()\n",
    "normal.load_state_dict(torch.load(\"/mnt/c/Users/papch/OneDrive/Υπολογιστής/Σχολή/Internship-Thesis/model_to_quantize_13092022.pth\"))\n",
    "\n",
    "normal = normal.to(device)\n",
    "next(normal.parameters()).device\n",
    "\n",
    "labels, predictions = inference(normal, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(predictions)):\n",
    "    predictions[i] = predictions[i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(labels)):\n",
    "    labels[i] = labels[i].item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cfm = sklearn.metrics.confusion_matrix(labels, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "cfmPlot = sklearn.metrics.ConfusionMatrixDisplay(cfm, display_labels=['healthy', 'crackle', 'wheeze', 'both'])\n",
    "cfmPlot.plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From now on we implement the quantization of the model.\n",
    "\n",
    "First part includes the dynamic quantization of the LSTM and Linear segment of the model.\n",
    "\n",
    "Second part includes the static quantzation of the CNN segment of the model.\n",
    "\n",
    "We can observe that after the quantization and reduction of the model size and inference time, the results are still quite good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#LSTM AND LINEAR DYNAMIC QUANTIZATION\n",
    "from torch.quantization import quantize_dynamic\n",
    "quantized_int8 = quantize_dynamic(\n",
    "    model=normal, qconfig_spec={nn.LSTM, nn.Linear}, dtype=torch.qint8, inplace=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1579, 649, 388, 145]\n",
      "[1235, 217, 76, 16]\n",
      "Test Accuracy: 0.5592, Sensitivity: 0.261, Specificity: 0.782, ICBHI Score: 0.522\n"
     ]
    }
   ],
   "source": [
    "labels2, predictions2 = inference(quantized_int8, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_size_of_model(model, label=\"\"):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    size=os.path.getsize(\"temp.p\")\n",
    "    print(\"model: \",label,' \\t','Size (KB):', size/1e3)\n",
    "    os.remove('temp.p')\n",
    "    return size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  normal  \t Size (KB): 673.108\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "673108"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_size_of_model(normal, label=\"normal\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  quant  \t Size (KB): 403.342\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "403342"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_size_of_model(quantized_int8, label=\"quant\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Combine2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rnn): DynamicQuantizedLSTM(288, 64, batch_first=True)\n",
       "  (linear): DynamicQuantizedLinear(in_features=64, out_features=4, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (quant): QuantStub()\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_int8.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fused = torch.quantization.fuse_modules(quantized_int8, [['conv1.0', 'conv1.2', 'conv1.1']])\n",
    "fused = torch.quantization.fuse_modules(quantized_int8, [['conv2.0', 'conv2.2', 'conv2.1']])\n",
    "fused = torch.quantization.fuse_modules(quantized_int8, [['conv3.0', 'conv3.2', 'conv3.1']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Combine2(\n",
       "  (conv1): Sequential(\n",
       "    (0): Conv2d(\n",
       "      1, 32, kernel_size=(5, 5), stride=(1, 1)\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(\n",
       "      32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): Conv2d(\n",
       "      32, 64, kernel_size=(3, 3), stride=(1, 1)\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(\n",
       "      64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): Conv2d(\n",
       "      64, 96, kernel_size=(3, 3), stride=(1, 1)\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (1): ReLU()\n",
       "    (2): BatchNorm2d(\n",
       "      96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True\n",
       "      (activation_post_process): HistogramObserver()\n",
       "    )\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rnn): DynamicQuantizedLSTM(288, 64, batch_first=True)\n",
       "  (linear): DynamicQuantizedLinear(in_features=64, out_features=4, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): HistogramObserver()\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "quantized_int8.qconfig = torch.quantization.get_default_qconfig(\"fbgemm\")\n",
    "torch.quantization.prepare(quantized_int8, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_set = []\n",
    "\n",
    "for i in range(100):\n",
    "    calibration_set.append(next(iter(train_dl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "\n",
    "with torch.inference_mode():\n",
    "    for data in calibration_set:\n",
    "        inputs, labels = data[0].to(device), data[1].to(device)\n",
    "        inputs = inputs.float()\n",
    "\n",
    "        results = quantized_int8(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Combine2(\n",
       "  (conv1): Sequential(\n",
       "    (0): QuantizedConv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), scale=2.6870713233947754, zero_point=70)\n",
       "    (1): ReLU()\n",
       "    (2): QuantizedBatchNorm2d(32, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv2): Sequential(\n",
       "    (0): QuantizedConv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), scale=1.7143712043762207, zero_point=96)\n",
       "    (1): ReLU()\n",
       "    (2): QuantizedBatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (conv3): Sequential(\n",
       "    (0): QuantizedConv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), scale=1.5027791261672974, zero_point=73)\n",
       "    (1): ReLU()\n",
       "    (2): QuantizedBatchNorm2d(96, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rnn): DynamicQuantizedLSTM(288, 64, batch_first=True)\n",
       "  (linear): DynamicQuantizedLinear(in_features=64, out_features=4, dtype=torch.qint8, qscheme=torch.per_tensor_affine)\n",
       "  (quant): Quantize(scale=tensor([0.8567]), zero_point=tensor([117]), dtype=torch.quint8)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "torch.quantization.convert(quantized_int8, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1579, 649, 388, 145]\n",
      "[1191, 193, 96, 17]\n",
      "Test Accuracy: 0.5422, Sensitivity: 0.259, Specificity: 0.754, ICBHI Score: 0.507\n"
     ]
    }
   ],
   "source": [
    "labels3, predictions3 = inference(quantized_int8, test_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model:  quant  \t Size (KB): 188.562\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "188562"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print_size_of_model(quantized_int8, label=\"quant\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
